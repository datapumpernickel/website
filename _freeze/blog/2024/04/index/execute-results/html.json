{
  "hash": "953ce9bd77a9c776ca131a84f27d5d7d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Part II: Patterns across versions of conflict data from UCDP\"\ndate: 2025-03-15\ndescription: \"Using the conflictoR package we query conflict data from the Uppsala Conflict Data Center and try to find patterns in retroactively changed data\"\nimage: \"index_files/figure-html/fig-1-replication-1.png\"\ncategories:\n  - r\n  - conflict data \nformat:\n  html: \n    shift-heading-level-by: 1\ninclude-before-body:\n  text: |\n    <style>\n      .no-stripe .gt_table tr.odd {\n        --bs-table-striped-bg: transparent;\n      }\n      \n      .gt_footnote {\n        text-align: left !important;\n      }\n    </style>\ncitation: true\nbibliography: ../02/references.bib\nnocite: |\n  @davies2024organized, @gleditsch2002armed, @eck2007violence, @sundberg2012introducing\n---\n\n\n\n\n**Please see [Part I](../02/index.qmd) of this blog entry, for an introduction to the topic. **\n\nI queried data from the [Uppsala Conflict Data Program (UCDP)](https://www.uu.se/en/department/peace-and-conflict-research/research/ucdp/). The different versions of their datasets do not only include updates to include the latest years, but also introduce corrections and changes to past conflicts. This of course is a great service and with the proper versioning, as they do, is not necessarily a problem for replicability. However, it also allows us to have an interesting insight into data availability and possible biases in conflict reporting. \n\n\n#### The conflictoR package\n\nSpecifically with the resource above, I have found that the two packages which promised to allow access to the API in R where not fully functional for my use-case (either because they did not allow filtering and contributions in a [private gitlab](https://gitlab.com/dante-sttr/conflictr) were complicated or because the package actually was [not fully developed](https://github.com/chris-dworschak/ucdp.api) and seemed stale). Hence I quickly wrote my own code, which I then packaged, so I can use it again in the future and others can use it to access the same data. \n\nYou can find the [conflictoR](https://github.com/datapumpernickel/conflictoR) package on github and install it as follows: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrequire(devtools)\ndevtools::install_github(\"datapumpernickel/conflictoR\")\n```\n:::\n\n\n## Visualizing the UCDP conflict counts\n\n\n## Tracking changes across different versions\n\nNow in order to look at how these numbers change across different versions, lets get the datasets for all versions. The below code uses `tidyr` to expand our unique combinations of datasets and versions to one dataset. We then use the handy `map2()` function from the `map()` family of `purrr` to iterate over the two columns of the data. We could have also achieved this with `pmap()`, but `map2()` makes it a bit more explicit how the variables were passed on. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## making a dataframe with all combinations of datasets and versions\nall_combinations  <- tidyr::expand_grid(\n  datasets  = c(\"battledeaths\", \"nonstate\", \"onesided\"),\n  versions  = c(\"24.1\", \"23.1\", \"22.1\", \"21.1\", \"20.1\", \"17.2\", \"18.1\", \"19.1\")\n)\n\n## querying this data using the conflictoR package\nfull_data <- map2(\n  all_combinations$datasets,\n  all_combinations$versions,\n  ~ cl_get_data(.x, .y) |>\n    mutate(\n      dataset = .x,\n      version = .y\n    )) \nwrite_rds(full_data, \"all_data.rds\")\n\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfull_data <- read_rds(\"all_data.rds\") |> \n  map_dfr(~.x |>\n    select(any_of(c(\"dataset\", \"version\", \"year\",\"type_of_conflict\", \"conflict_id\", \"location\",\"bd_best\",\"best_fatality_estimate\"))) |> \n      mutate(across(any_of(c(\"year\",\"best_fatality_estimate\")), as.numeric))\n)\n\n```\n:::\n\n\nNow that we queried all the dataset-version combinations, we can bind the datasets together and actually count the conflicts in each year for each version and dataset. For each dataset-year combination, we also get the maximum and minimum count, to be able to plot this range as well. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## clean the data to count actual conflicts\nclean_full_data <- full_data |>\n  reduce(bind_rows) |>\n  count(year, version, dataset) |> \n  group_by(year, dataset) |> \n  mutate(n_max = max(n),\n         n_min = min(n)) |> \n  mutate(year = as.numeric(year))\n```\n:::\n\n\n::: {.panel-tabset}\n\n## Plot\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Changing counts of conflict episodes across different datasets and versions](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n## Code\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\n## making a dataframe with all combinations of datasets and versions\nall_combinations  <- tidyr::expand_grid(\n  datasets  = c(\"ucdpprioconflict\", \"nonstate\", \"onesided\"),\n  versions  = c(\"24.1\", \"23.1\", \"22.1\", \"21.1\", \"20.1\", \"17.2\", \"18.1\", \"19.1\")\n)\n\n## querying this data using the conflictoR package\nfull_data <- map2(\n  all_combinations$datasets,\n  all_combinations$versions,\n  ~ cl_get_data(.x, .y) |>\n    mutate(\n      dataset = .x,\n      year = as.character(year),\n      conflict_id,\n      version = .y\n    )\n)\n\n## clean the data to count actual conflicts\nclean_full_data <- full_data |>\n  map( ~ .x |> distinct(conflict_id, year, version, dataset)) |>\n  reduce(bind_rows) |>\n  count(year, version, dataset) |> \n  group_by(year, dataset) |> \n  mutate(n_max = max(n),\n         n_min = min(n)) |> \n  mutate(year = as.numeric(year))\n\nggplot(data = clean_full_data |> \n         filter(year %in% 2000:2024)) +\n  geom_ribbon(aes(x = year, ymax = n_max, ymin = n_min), \n                fill = \"grey80\")+\n  geom_point(aes(x = year, y = n,group =version, color = version),\n             alpha = 0.7)+\n  labs(title = \"Changes in conflict counts\", subtitle =\"over different versions of yearly totals\")+\n  facet_wrap(.~dataset, ncol = 1)+\n  theme_minimal()+\n  scale_x_continuous(breaks = seq(2000, 2024, 2))+\n    scale_color_brewer(palette = \"Dark2\")\n```\n:::\n\n:::\n\nIt becomes quite evident that not too seldomly the number of conflicts is changed from one version to another, even if the actual conflict is quite some years back! This should probably not come as a surprise, since areas of violent conflict are usually hard to gain access to and collecting information in this environment is dangerous and news articles about these incidents are probably often not accurate, until some time has passed to allow for thorough investigation. It speaks to the dedication of researchers at UCDP that they incorporate changes so far back into their datasets. \n\nThe fact that we can see the largest amount of changes in the last 10 years, might be due to the fact, that the API only allows us to query the last 7 versions dating back to 2017. \n\n#### Overestimation or Underestimation?\n\nIntuitively, one would assume that lack of information leads to a systematic underestimation of conflict data. \nHence, one question I wanted to answer was: **Are conflicts usually underestimated and then corrected upwards or are conflict periods also overestimated?**\n\nTo answer this question, for each dataset I compared the changes between current and previous version and checked if the previous year-dataset-version triad was smaller than the current one or if there was no change at all. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code to clean and prepare the data\"}\nchanges_analysis <- clean_full_data |> \n  group_by(dataset, year) |> \n  arrange(dataset, year, version) |> \n  mutate(lag_n = lag(n),\n         biased_downward = n>lag_n,\n         perc_change = ((n/lag_n)-1)*100,\n         change = n!= lag_n) |> \n  filter(change) |> \n  ungroup()\n```\n:::\n\n\nI found 167 periods were data was corrected ex post. A majority, in total 153 of these were corrected upwards. In addition, we can see that more than half of these episodes experience a change of roundabout 2.5 - 5% change upwards, whereas there is some really stark outliers, where the number of conflicts changes by a staggering 20%. \n\n::: {.panel-tabset}\n\n## Plot\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Direction of change in corrections across different versions](index_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Code\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork)\np1 <- ggplot(changes_analysis)+\n  geom_boxplot(aes(biased_downward, perc_change)) +\n    geom_hline(aes(yintercept = 0), color= \"#bd5b54\", alpha = 0.8)+\n  theme_minimal()+\n  labs(\n       x = \"\", \n       y = \"Change in % of original value\",\n       caption = \"Uppsala Conflict Data Program, ucdp.uu.se, version 17.2 through 24.1 of resources: 'onesided', 'ucdpprioconflict' and 'nonstate'\")+\n  theme(plot.caption = element_text(face = \"italic\",size = 7 ))\n\np2 <- ggplot(changes_analysis |> count(biased_downward))+\n  geom_col(aes(x = 1, y = n, group = biased_downward, fill = biased_downward), position = \"stack\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(legend.position = \"top\",\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.title = element_text(size = 9),\n        legend.text = element_text(size = 8))+\n  scale_fill_manual(values = c(\"#e7a169\",\"#846e9a\"))+\n  labs(title = \"Corrections in percent of original value\", \n       subtitle = \"by directional change\", fill = \"Counts were corrected upwards\")\n\np2 / p1 + plot_layout(heights = c(1,5))\n```\n:::\n\n\n:::\n\n\n## Conclusion\n\nWe saw that it is quite easy to provide some interesting insights into how conflict episodes changed retrospectively, but this was only so straight forward, because UCDP on the one hand provides an API for easy access to their data and secondly, very meticulously versions different versions of their dataset. The takeaway from this hence should be: \n\na) be careful when interpreting this data, you might be underestimating conflict episodes by a few percent across the board \n\nand \n\nb) always make sure you make transparent which version of a dataset you are using, so others can replicate your analysis (and of course attribute the source). \n\nThis seems straight forward, but large data providers, such as one of the single most important sources for Trade Data, [UN Comtrade](https://comtradeplus.un.org), do not version their datasets in this way and you might never be able to replicate your analysis, if you did not make a snapshot of the data by yourself and share it with others. \n\n\n---\nnocite: |\n  @davies2024organized, @gleditsch2002armed, @eck2007violence, @sundberg2012introducing\n---\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}